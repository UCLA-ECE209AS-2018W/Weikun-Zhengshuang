{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split datasets into train, validation, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module can use for processing split datasets. You need modify the ratio of train, validation, and test. And you can modify output directory you want and input directory you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will send .csv dataset to ./datasets/final_dataset\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" Split datasets into train, validation, and test\n",
    "\n",
    "This module can use for processing split datasets. You need modify the ratio of \n",
    "train, validation, and test datasetes. And you can modify output directory you \n",
    "want and  input directory you have.\n",
    "\n",
    "################################################################################\n",
    "# Author: Weikun Han <weikunhan@gmail.com>\n",
    "# Crate Date: 03/6/2018        \n",
    "# Update:\n",
    "# Reference: https://github.com/jhetherly/EnglishSpeechUpsampler\n",
    "################################################################################\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def write_csv(filename, pairs):\n",
    "    \"\"\"The function to wirte\n",
    "\n",
    "    Args:\n",
    "        param1 (str): filename \n",
    "        param2 (list): pairs\n",
    "\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        for n in pairs:\n",
    "            writer.writerow(n)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Please modify input path  to locate you file\n",
    "    DATASETS_ROOT_DIR = './datasets'\n",
    "    OUTPUT_DIR = os.path.join(DATASETS_ROOT_DIR, 'final_dataset')\n",
    "\n",
    "    # Define ratio for train, validation, and test datasetes\n",
    "    train_fraction = 0.6\n",
    "    validation_fraction = 0.2\n",
    "    test_fraction = 0.2\n",
    "\n",
    "    # Reset random generator\n",
    "    np.random.seed(0)    \n",
    "    \n",
    "    # Check location to save datasets\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        \n",
    "    print('Will send .csv dataset to {}'.format(OUTPUT_DIR))\n",
    "    \n",
    "    # Create list to store each original and noise file name pair\n",
    "    original_noise_pairs = []\n",
    "    input_original_path = os.path.join(DATASETS_ROOT_DIR, 'TEDLIUM_5S')\n",
    "    input_noise_path = os.path.join(DATASETS_ROOT_DIR, \n",
    "                                    'TEDLIUM_noise_sample_5S')\n",
    "\n",
    "    for filename in os.listdir(input_original_path):\n",
    "        \n",
    "        # Link same filename in noise path\n",
    "        filename_component = filename.split('_')\n",
    "        filename_noise = (filename_component[0] +\n",
    "                          '_' +\n",
    "                          filename_component[1] +\n",
    "                          '_' +\n",
    "                          'noise_sample' +\n",
    "                          '_' +\n",
    "                          filename_component[2])\n",
    "        input_original_filename = os.path.join(input_original_path, \n",
    "                                               filename)\n",
    "        input_noise_filename = os.path.join(input_noise_path, filename_noise)\n",
    "        \n",
    "        if not os.path.isfile(input_original_filename):\n",
    "            continue\n",
    "        \n",
    "        original_noise_pairs.append(\n",
    "            [input_original_filename, input_noise_filename])\n",
    "\n",
    "    # Shuffle the datasets\n",
    "    np.random.shuffle(original_noise_pairs)\n",
    "    datasets_size = len(original_noise_pairs)\n",
    "    \n",
    "    # Create indexs\n",
    "    validation_start_index = 0\n",
    "    validation_end_index = (validation_start_index + \n",
    "                            int(datasets_size * validation_fraction))\n",
    "    test_start_index = validation_end_index\n",
    "    test_end_index = (test_start_index + \n",
    "                      int(datasets_size * test_fraction))\n",
    "    train_start_index = test_end_index\n",
    "    \n",
    "    # Save pairs into .csv\n",
    "    validation_original_noise_pairs = original_noise_pairs[\n",
    "        validation_start_index:validation_end_index]\n",
    "    write_csv(os.path.join(OUTPUT_DIR, 'validation_files.csv'),\n",
    "              validation_original_noise_pairs)\n",
    "    test_original_noise_pairs = original_noise_pairs[\n",
    "        test_start_index : test_end_index]\n",
    "    write_csv(os.path.join(OUTPUT_DIR, 'test_files.csv'), \n",
    "              test_original_noise_pairs)\n",
    "    train_original_noise_pairs = original_noise_pairs[\n",
    "        train_start_index :]\n",
    "    write_csv(os.path.join(OUTPUT_DIR, 'train_files.csv'), \n",
    "              original_noise_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
